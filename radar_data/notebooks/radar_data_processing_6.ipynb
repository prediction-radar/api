{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nRaN7EsWE-Na"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from glob import glob\n",
        "from matplotlib.colors import LinearSegmentedColormap, Normalize\n",
        "from matplotlib import cm\n",
        "\n",
        "\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', 1000)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "root_dir = \"/Users/trevorwiebe/Ktor/radar_backend/radar_data/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the .npy files into a list\n",
        "npy_files = sorted(glob(root_dir + 'data/npy_files/*.npy'))\n",
        "\n",
        "# Load the first .npy file to get the unique latitudes and longitudes of this data set\n",
        "example_grid = np.load(npy_files[0])\n",
        "unique_latitudes = example_grid.shape[1]\n",
        "unique_longitudes = example_grid.shape[2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in enumerate(npy_files):\n",
        "    array = np.load(i[1])\n",
        "    \n",
        "    array[array < 25] = 0\n",
        "\n",
        "    np.save(i[1], array)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Assume npy_files and unique_latitudes, unique_longitudes are already defined\n",
        "sequence_length = 10  # Number of .npy files to look back on\n",
        "prediction_horizon = 10  # Number of .npy files to predict\n",
        "height, width = unique_latitudes, unique_longitudes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to split a single grid into 250x250 squares\n",
        "def split_single_grid(file, square_size=250, height=3500, width=7000):\n",
        "    grid = np.load(file).reshape(height, width, 1)\n",
        "    squares = [\n",
        "        grid[i:i+square_size, j:j+square_size]\n",
        "        for i in range(0, height, square_size)\n",
        "        for j in range(0, width, square_size)\n",
        "    ]\n",
        "    return np.array(squares)\n",
        "\n",
        "# Function to split all grids sequentially (no parallelization)\n",
        "def split_all_grids_sequential(npy_files, square_size=250, height=3500, width=7000):\n",
        "    num_squares_per_grid = (height // square_size) * (width // square_size)\n",
        "    num_files = len(npy_files)\n",
        "\n",
        "    # Preallocate space for all the grids that will be split into squares\n",
        "    all_squares = np.empty((num_files, num_squares_per_grid, square_size, square_size, 1), dtype=np.float32)\n",
        "\n",
        "    # Sequentially process each file and store the result\n",
        "    for idx, file in enumerate(npy_files):\n",
        "        squares = split_single_grid(file, square_size, height, width)\n",
        "        all_squares[idx] = squares\n",
        "\n",
        "    return all_squares\n",
        "\n",
        "# Assume npy_files, sequence_length, and prediction_horizon are already defined\n",
        "square_size = 500\n",
        "height, width = 3500, 7000  # Original image size\n",
        "num_squares = 98\n",
        "\n",
        "# Pre-split all grids into squares sequentially\n",
        "all_squares = split_all_grids_sequential(npy_files, square_size=square_size, height=height, width=width)\n",
        "\n",
        "# Now, treat each square as an independent sample\n",
        "num_files = len(npy_files)\n",
        "num_samples = (num_files - sequence_length - prediction_horizon + 1) * num_squares\n",
        "print(f\"Number of samples: {num_samples}\")\n",
        "\n",
        "# Pre-allocate arrays for input (X) and output (y) sequences\n",
        "X = np.empty((num_samples, sequence_length, square_size, square_size, 1), dtype=np.float32)\n",
        "y = np.empty((num_samples, prediction_horizon, square_size, square_size, 1), dtype=np.float32)\n",
        "\n",
        "# Populate X and y arrays using pre-split squares\n",
        "sample_idx = 0\n",
        "for i in range(num_files - sequence_length - prediction_horizon + 1):\n",
        "    for j in range(num_squares):\n",
        "        # For each sample, take the sequence for that square over time\n",
        "        X[sample_idx] = all_squares[i:i+sequence_length, j]\n",
        "        y[sample_idx] = all_squares[i+sequence_length:i+sequence_length+prediction_horizon, j]\n",
        "        sample_idx += 1\n",
        "\n",
        "# Split data into train, validation, and test sets\n",
        "train_ratio = 0.7\n",
        "val_ratio = 0.15\n",
        "test_ratio = 0.15\n",
        "\n",
        "# First split: train and temp (which will later be split into validation and test)\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    X, y, test_size=(1 - train_ratio), random_state=42, shuffle=True\n",
        ")\n",
        "\n",
        "# Second split: validation and test\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    X_temp, y_temp, test_size=(test_ratio / (test_ratio + val_ratio)), random_state=42, shuffle=True\n",
        ")\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_val.shape)\n",
        "print(X_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_val.shape)\n",
        "print(y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the colors for the radar map, introducing white for values between 0 and 15\n",
        "colors = [\n",
        "    (0, 0, 0),         # White for values 0-15 (no precipitation or very light)\n",
        "    (0, 0.7, 0),       # Green (light precipitation)\n",
        "    (1, 1, 0),         # Yellow (moderate precipitation)\n",
        "    (1, 0.65, 0),      # Orange (heavy precipitation)\n",
        "    (1, 0, 0),         # Red (very heavy precipitation)\n",
        "    (0.6, 0, 0.6)      # Purple (extreme precipitation)\n",
        "]\n",
        "\n",
        "# Define the breakpoints for the color transition, where 0-15 is white\n",
        "breakpoints = [0.0, 15/80.0, 40/80.0, 60/80.0, 70/80.0, 1.0]\n",
        "\n",
        "# Create the custom colormap\n",
        "radar_cmap = LinearSegmentedColormap.from_list('radar', colors, N=256)\n",
        "\n",
        "# Normalize the data range from 0 to 80\n",
        "norm = Normalize(vmin=0, vmax=80)\n",
        "\n",
        "# Construct a figure to visualize the images\n",
        "fig, axes = plt.subplots(5, 2, figsize=(20, 16))\n",
        "\n",
        "# Randomly choose a data example to visualize\n",
        "data_choice = np.random.choice(range(len(X_train)), size=1)[0]\n",
        "lastdata = None  # Initialize lastdata to None before looping\n",
        "\n",
        "# Plot each of the sequential images for one random data example\n",
        "for idx, ax in enumerate(axes.flat):\n",
        "    thisdata = X_train[data_choice][idx]\n",
        "\n",
        "    # Compare the current data to the last one if lastdata is not None\n",
        "    if lastdata is not None and np.array_equal(thisdata, lastdata):\n",
        "        print(f\"Frame {idx + 1} is the same as the previous frame.\")\n",
        "\n",
        "    # Display the image with the radar colormap and normalization\n",
        "    im = ax.imshow(np.squeeze(thisdata), cmap=radar_cmap, norm=norm)\n",
        "    \n",
        "    # Add title and remove axis\n",
        "    ax.set_title(f\"Frame {idx + 1}\")\n",
        "    ax.axis(\"off\")\n",
        "\n",
        "    # Update lastdata\n",
        "    lastdata = thisdata\n",
        "\n",
        "# Print information and display the figure\n",
        "print(f\"Displaying frames for example {data_choice}.\")\n",
        "\n",
        "# Display colorbar to show the mapping of values to the radar colors\n",
        "fig.colorbar(im, ax=axes.ravel().tolist(), shrink=0.95)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import ConvLSTM2D, Conv3D, BatchNormalization, Input\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras import backend as K\n",
        "import io\n",
        "import imageio\n",
        "from IPython.display import Image, display\n",
        "from ipywidgets import widgets, Layout, HBox\n",
        "\n",
        "channels = 1  # Reflectivity is your feature, so 1 channel\n",
        "\n",
        "# Define the model using an Input layer for the input shape\n",
        "model = Sequential()\n",
        "\n",
        "# Add Input Layer\n",
        "model.add(Input(shape=(sequence_length, square_size, square_size, channels))) \n",
        "\n",
        "# First ConvLSTM2D layer with return_sequences=True\n",
        "model.add(ConvLSTM2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same', return_sequences=True))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# Second ConvLSTM2D layer with return_sequences=True to return all frames\n",
        "model.add(ConvLSTM2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same', return_sequences=True))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# Replace Conv3D with Conv2D to predict the next frame(s)\n",
        "model.add(Conv3D(filters=1, kernel_size=(3, 3, 3), activation='linear', padding='same'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='mse', optimizer='adam')\n",
        "\n",
        "# Print model summary\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_model_memory_usage(batch_size, model):\n",
        "    features_mem = 0  # Initialize memory for features\n",
        "    float_bytes = 4.0  # Float32 uses 4 bytes\n",
        "    \n",
        "    for layer in model.layers:\n",
        "        # Use layer.output.shape to get the output shape instead of output_shape\n",
        "        out_shape = layer.output.shape\n",
        "        \n",
        "        # Remove the batch size dimension (out_shape[0]) and None (which represents the batch dimension)\n",
        "        out_shape = [dim for dim in out_shape if dim is not None]\n",
        "        \n",
        "        # Multiply all output shape dimensions to calculate the number of elements per layer\n",
        "        single_layer_mem = 1\n",
        "        for s in out_shape:\n",
        "            single_layer_mem *= s\n",
        "            \n",
        "        # Convert to memory (in bytes and MB)\n",
        "        single_layer_mem_float = single_layer_mem * float_bytes  # Multiply by 4 bytes (float32)\n",
        "        single_layer_mem_MB = single_layer_mem_float / (1024 ** 2)  # Convert to MB\n",
        "        \n",
        "        print(f\"Memory for layer {layer.name} with output shape {out_shape} is: {single_layer_mem_MB:.2f} MB\")\n",
        "        \n",
        "        features_mem += single_layer_mem_MB  # Accumulate total feature memory\n",
        "    \n",
        "    # Calculate Parameter memory\n",
        "    trainable_wts = np.sum([K.count_params(p) for p in model.trainable_weights])\n",
        "    non_trainable_wts = np.sum([K.count_params(p) for p in model.non_trainable_weights])\n",
        "    parameter_mem_MB = ((trainable_wts + non_trainable_wts) * float_bytes) / (1024 ** 2)\n",
        "    \n",
        "    print(\"_________________________________________\")\n",
        "    print(f\"Memory for features in MB is: {features_mem * batch_size:.2f} MB\")\n",
        "    print(f\"Memory for parameters in MB is: {parameter_mem_MB:.2f} MB\")\n",
        "\n",
        "    total_memory_MB = (batch_size * features_mem) + parameter_mem_MB\n",
        "    total_memory_GB = total_memory_MB / 1024  # Convert to GB\n",
        "    \n",
        "    return total_memory_GB\n",
        "\n",
        "#####################################################################\n",
        "\n",
        "mem_for_my_model = get_model_memory_usage(1, model)\n",
        "\n",
        "print(\"_________________________________________\")\n",
        "print(\"Minimum memory required to work with this model is: %.2f\" %mem_for_my_model, \"GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# model = load_model(root_dir + 'model/model6_2.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cp = ModelCheckpoint(root_dir + 'model/model6_3.keras', save_best_only=True)\n",
        "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", patience=5)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, \n",
        "          batch_size=2, \n",
        "          epochs=20, \n",
        "          callbacks=[cp, early_stopping, reduce_lr],\n",
        "          validation_data=(X_val, y_val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select a random example from the validation dataset.\n",
        "example = X_val[np.random.choice(range(len(X_val)), size=1)[0]]\n",
        "\n",
        "# Pick the first/last ten frames from the example.\n",
        "frames = example[:10, ...]\n",
        "original_frames = example[10:, ...]\n",
        "\n",
        "# Predict a new set of 10 frames.\n",
        "for _ in range(10):\n",
        "    # Extract the model's prediction and post-process it.\n",
        "    new_prediction = model.predict(np.expand_dims(frames, axis=0))\n",
        "    new_prediction = np.squeeze(new_prediction, axis=0)\n",
        "    predicted_frame = np.expand_dims(new_prediction[-1, ...], axis=0)\n",
        "\n",
        "    # Extend the set of prediction frames.\n",
        "    frames = np.concatenate((frames, predicted_frame), axis=0)\n",
        "\n",
        "# Construct a figure for the original and new frames.\n",
        "fig, axes = plt.subplots(2, 10, figsize=(20, 4))\n",
        "\n",
        "# Plot the original frames.\n",
        "for idx, ax in enumerate(axes[0]):\n",
        "    ax.imshow(np.squeeze(original_frames[idx]), cmap=\"Blues\")\n",
        "    ax.set_title(f\"Frame {idx + 11}\")\n",
        "    ax.axis(\"off\")\n",
        "\n",
        "# Plot the new frames.\n",
        "new_frames = frames[10:, ...]\n",
        "for idx, ax in enumerate(axes[1]):\n",
        "    ax.imshow(np.squeeze(new_frames[idx]), cmap=\"BuGn\")\n",
        "    ax.set_title(f\"Frame {idx + 11}\")\n",
        "    ax.axis(\"off\")\n",
        "\n",
        "# Display the figure.\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select a few random examples from the dataset.\n",
        "examples = X_test[np.random.choice(range(len(X_test)), size=1)]\n",
        "\n",
        "\n",
        "# Iterate over the examples and predict the frames.\n",
        "predicted_videos = []\n",
        "for example in examples:\n",
        "    # Pick the first/last ten frames from the example.\n",
        "    frames = example[:10, ...]\n",
        "    original_frames = example[10:, ...]\n",
        "    new_predictions = np.zeros(shape=(10, *frames[0].shape))\n",
        "\n",
        "    # Predict a new set of 10 frames.\n",
        "    for i in range(10):\n",
        "        # Extract the model's prediction and post-process it.\n",
        "        frames = example[: 10 + i + 1, ...]\n",
        "        new_prediction = model.predict(np.expand_dims(frames, axis=0))\n",
        "        new_prediction = np.squeeze(new_prediction, axis=0)\n",
        "        predicted_frame = np.expand_dims(new_prediction[-1, ...], axis=0)\n",
        "\n",
        "        # Extend the set of prediction frames.\n",
        "        new_predictions[i] = predicted_frame\n",
        "\n",
        "    # Create and save GIFs for each of the ground truth/prediction images.\n",
        "    for frame_set in [original_frames, new_predictions]:\n",
        "        # Construct a GIF from the selected video frames.\n",
        "        current_frames = np.squeeze(frame_set)\n",
        "        current_frames = current_frames[..., np.newaxis] * np.ones(3)\n",
        "        current_frames = (current_frames * 255).astype(np.uint8)\n",
        "        current_frames = list(current_frames)\n",
        "\n",
        "        # Construct a GIF from the frames.\n",
        "        with io.BytesIO() as gif:\n",
        "            imageio.mimsave(gif, current_frames, \"GIF\", duration=200)\n",
        "            predicted_videos.append(gif.getvalue())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display the videos.\n",
        "print(\" Truth\\tPrediction\")\n",
        "for i in range(0, len(predicted_videos), 2):\n",
        "    # Construct and display an `HBox` with the ground truth and prediction.\n",
        "    box = HBox(\n",
        "        [\n",
        "            widgets.Image(value=predicted_videos[i]),\n",
        "            widgets.Image(value=predicted_videos[i + 1]),\n",
        "        ]\n",
        "    )\n",
        "    display(box)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
