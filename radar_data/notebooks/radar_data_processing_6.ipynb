{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DpClNkF0MQTD",
        "outputId": "629f9b92-b835-4432-a6a1-9859e26bf5c6"
      },
      "outputs": [],
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "nRaN7EsWE-Na"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', 1000)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "root_dir = \"/Users/trevorwiebe/Ktor/radar_backend/radar_data/\"\n",
        "\n",
        "data = pd.read_csv(root_dir + 'data/combined_data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "def transform_dataframe(df, block_size):\n",
        "    num_blocks = len(df) // block_size  # Number of complete blocks of block_size rows\n",
        "    num_fields = block_size - 1  # Number of rows before the target row\n",
        "\n",
        "    # Preallocate the dataframe for speed\n",
        "    columns = (\n",
        "        [f'dateTime_t', f'latitude_t', f'longitude_t', f'reflectivity_t'] +\n",
        "        [item for i in range(1, num_fields + 1) for item in [f'dateTime_{i}', f'latitude_{i}', f'longitude_{i}', f'reflectivity_{i}']]\n",
        "    )\n",
        "    data = np.empty((num_blocks, len(columns)), dtype=object)  # Preallocate as object array\n",
        "\n",
        "    # Convert DataFrame to numpy array for faster processing\n",
        "    arr = df[['dateTime', 'latitude', 'longitude', 'reflectivity']].values\n",
        "\n",
        "    # Process the blocks in bulk\n",
        "    for i in range(num_blocks):\n",
        "        start = i * block_size\n",
        "        end = start + block_size\n",
        "\n",
        "        # Use the last row in the block (target row)\n",
        "        data[i, 0:4] = arr[end-1]  # Target row: dateTime_t, latitude_t, longitude_t, reflectivity_t\n",
        "\n",
        "        # Use the first (block_size - 1) rows (1- (block_size - 1) rows)\n",
        "        for j in range(num_fields):\n",
        "            data[i, 4 + j*4] = arr[start+j, 0]  # dateTime_j+1\n",
        "            data[i, 5 + j*4] = arr[start+j, 1]  # latitude_j+1\n",
        "            data[i, 6 + j*4] = arr[start+j, 2]  # longitude_j+1\n",
        "            data[i, 7 + j*4] = arr[start+j, 3]  # reflectivity_j+1\n",
        "\n",
        "    # Create the final DataFrame\n",
        "    transformed_df = pd.DataFrame(data, columns=columns)\n",
        "    \n",
        "    return transformed_df\n",
        "\n",
        "df = transform_dataframe(data, 31)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "1ppnxhzYN56N"
      },
      "outputs": [],
      "source": [
        "# Setting device\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Assuming 'df' is your original dataframe\n",
        "reflectivity_columns = [f'reflectivity_{i}' for i in range(1, 31)]  # List of reflectivity columns\n",
        "\n",
        "# Set reflectivity between -99.0 and -1.0 to 0 in both historical and target reflectivity columns\n",
        "df[reflectivity_columns] = df[reflectivity_columns].map(lambda x: 0 if -99.0 <= x <= -1.0 else x)\n",
        "df['reflectivity_t'] = df['reflectivity_t'].apply(lambda x: 0 if -99.0 <= x <= -1.0 else x)\n",
        "\n",
        "# Define the number of timesteps (30) and features (latitude, longitude, minute)\n",
        "timesteps = 30\n",
        "features = 6  # latitude, longitude, minute, hour, month, reflectivity\n",
        "\n",
        "# Create an array to hold the input sequences\n",
        "X = np.zeros((len(df), timesteps, features))\n",
        "\n",
        "# Convert the dateTime columns to datetime objects to extract the minute\n",
        "for t in range(1, timesteps + 1):\n",
        "    df[f'dateTime_{t}'] = pd.to_datetime(df[f'dateTime_{t}'])\n",
        "\n",
        "# Loop through each timestep and fill in the features (latitude, longitude, minute)\n",
        "for t in range(1, timesteps + 1):\n",
        "    X[:, t - 1, 0] = df[f'latitude_{t}']   # latitude\n",
        "    X[:, t - 1, 1] = df[f'longitude_{t}']  # longitude\n",
        "    X[:, t - 1, 2] = df[f'dateTime_{t}'].dt.hour   # hour of the day\n",
        "    X[:, t - 1, 3] = df[f'dateTime_{t}'].dt.minute  # minute of the hour\n",
        "    X[:, t - 1, 4] = df[f'dateTime_{t}'].dt.month # month of the year\n",
        "    X[:, t - 1, 5] = df[f'reflectivity_{t}'] \n",
        "\n",
        "# Target value: reflectivity_t (the value we want to predict)\n",
        "y = df['reflectivity_t'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Reshape X from 3D to 2D\n",
        "n_samples, n_timesteps, n_features = X.shape  # (675700, 30, 6)\n",
        "X = X.reshape(-1, n_features)  # Reshape to (675700 * 30, 6)\n",
        "\n",
        "# Step 2: Apply MinMaxScaler\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)  # Scale each feature\n",
        "\n",
        "# Step 3: Reshape back to the original 3D shape\n",
        "X = X.reshape(n_samples, n_timesteps, n_features)\n",
        "\n",
        "# For the target y, you can also scale it if needed:\n",
        "y = scaler.fit_transform(y.reshape(-1, 1)).flatten()  # Rescale y and flatten"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0EB0J8EIRfo",
        "outputId": "96a47cbb-3619-4101-cecc-02fb037eb19e"
      },
      "outputs": [],
      "source": [
        "# Create train, val and test splits\n",
        "\n",
        "train_split = int(X.shape[0] * .8)\n",
        "val_split = int(X.shape[0] * .9)\n",
        "\n",
        "X_train, y_train = X[:train_split], y[:train_split]\n",
        "X_val, y_val = X[train_split:val_split], y[train_split:val_split]\n",
        "X_test, y_test = X[val_split:], y[val_split:]\n",
        "\n",
        "X_train = np.array(X_train, dtype=np.float32)\n",
        "X_val = np.array(X_val, dtype=np.float32)\n",
        "X_test = np.array(X_test, dtype=np.float32)\n",
        "y_train = np.array(y_train, dtype=np.float32)\n",
        "y_val = np.array(y_val, dtype=np.float32)\n",
        "y_test = np.array(y_test, dtype=np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "yYfYQ2KIIRfp",
        "outputId": "cf66639f-048c-4eb2-af35-abc271c83399"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "from tensorflow.keras.metrics import RootMeanSquaredError\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# # Use this if starting from scratch\n",
        "# model1 = Sequential()\n",
        "# model1.add(InputLayer((30, 6)))\n",
        "# # LSTM layer with 64 units and dropout for regularization\n",
        "# model1.add(LSTM(64, return_sequences=False))  # return_sequences=False because we predict one value\n",
        "# model1.add(Dropout(0.2))  # Helps prevent overfitting\n",
        "\n",
        "# # Dense layer for additional feature extraction\n",
        "# model1.add(Dense(32, activation='relu'))  # Increased neurons for more complexity\n",
        "# model1.add(Dropout(0.2))  # More dropout\n",
        "\n",
        "# # Final output layer (predicting a single value)\n",
        "# model1.add(Dense(1, activation='linear'))\n",
        "\n",
        "# model1.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "BZhwltPXIRfp"
      },
      "outputs": [],
      "source": [
        "model1 = load_model(root_dir + 'model/model6.keras')\n",
        "cp = ModelCheckpoint(root_dir + 'model/model6.keras', save_best_only=True)\n",
        "model1.compile(loss=MeanSquaredError(), optimizer=SGD(learning_rate=0.001), metrics=[RootMeanSquaredError()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "zcLQN98eIRfp",
        "outputId": "18ddf59f-f988-491f-b2f0-c13801b91f8b"
      },
      "outputs": [],
      "source": [
        "model1.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=4, callbacks=[cp])\n",
        "print(\"Finished training at \" + datetime.now().strftime('%d/%m/%y %H:%M:%S.%f'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-1aw8JrJIRfp",
        "outputId": "dc478fa8-e3ab-4db1-ef23-872dcd9b3afa"
      },
      "outputs": [],
      "source": [
        "def make_predictions(X_pred, y_pred):\n",
        "    test_predictions = model1.predict(X_pred).flatten()\n",
        "    # third_column = np.flip(X_pred[:, :, X_pred.shape[2]-1], axis=1)\n",
        "    third_column = X_pred[:, :, X_pred.shape[2]-1]\n",
        "    X_test_strings = [' '.join(map(str, row)) for row in third_column]\n",
        "    test_results = pd.DataFrame(data={'Historical':X_test_strings, 'Actuals':y_pred, 'Val Predictions':test_predictions,})\n",
        "    # sorted_results = test_results.sort_values(by='Actuals', ascending=False)\n",
        "    return test_results[:500]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_test_predictions = make_predictions(X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_data(returned_data):\n",
        "    actuals = returned_data['Actuals']\n",
        "    predictions = returned_data['Val Predictions']\n",
        "\n",
        "    x_positions = np.linspace(0, 20, len(predictions))  # Spread x data across the range 0 to 10\n",
        "    y_positions = np.linspace(0, 20, len(actuals))  # Spread y data across the range 0 to 10\n",
        "\n",
        "    # Plotting the line graph\n",
        "    plt.plot(y_positions, actuals, label='Actual', color='green')   # Plot y with its new positions\n",
        "    plt.plot(x_positions, predictions, label='Predictions', color='red')  # Plot x with its new positions\n",
        "\n",
        "    plt.rcParams[\"figure.figsize\"] = (20,10)\n",
        "\n",
        "    # Adding labels, title, and legend\n",
        "    plt.xlabel('Spread-Out X-axis')\n",
        "    plt.ylabel('Value')\n",
        "    plt.title('Spreading Data Across the Graph')\n",
        "    plt.legend()  # Show the legend\n",
        "\n",
        "    # Show the graph\n",
        "    plt.show()\n",
        "\n",
        "plot_data(X_test_predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_val_predictions = make_predictions(X_val, y_val)\n",
        "plot_data(X_val_predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train_predictions = make_predictions(X_train, y_train)\n",
        "plot_data(X_train_predictions)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
